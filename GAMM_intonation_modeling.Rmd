---
title: 'Modeling intonational tunes with Generalized Additive Mixed Models: A practical
  introduction'
author: "Jeremy Steffman"
date: " last updated `r format(Sys.time(), '%B, %Y')`"
output:
  html_document: 
      toc: true
      toc_float: true
      toc_depth: 3
  pdf_document: default
  
bibliography: ref.bib
csl: apa.csl
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

```{r,echo=FALSE,message=FALSE}
library(ggplot2)
library(tidyverse)
library(forecast)
library(cowplot)
data<-read.csv("GAMM_demo_data.csv")
data %>%select(-X) ->data
targets<-read.csv("targets.csv")
data %>% group_by(tune,prop_dur) %>% mutate(erbc_mean= mean(erbc)) %>%slice(1) %>% select(tune,prop_dur,erbc_mean)->data_means
theme_set(theme_bw(base_size = 13))
```



## Introduction

This introduction is aimed at outlining the basics of GAM(M) modeling and applying them to modeling intonational tunes in American English. It draws heavily on previous tutorials and introductions, particularly @soskuthy2017generalised. I assume some basic familiarity with linear models. Familiarity with R is a plus for reading this document, as some code snippets are included, but probably not needed. 

### How to use this tutorial
If you want to just read this document you will get an introduction to GAM(M)s and an example of their application in intonation research. 

If you want to get more hands on, you can access the corresponding data and RMarkdown document to experiment with the code, and fit your own models with the data. You can find this [here](https://github.com/jsteffman/gamm-intro). 

This is intended to be a mostly conceptual introduction -

* For more in-depth descriptions of GAM(M)s see, e.g. @soskuthy2021evaluating and @wood2017generalized. 

* The document also includes notes for <span style="color: purple;">further reading</span> where relevant. 

## The data  

The data we will use comes from an imitative speech paradigm in which speakers listened to model utterances such as "Her name is Marilyn", with f0 resynthesized to create various intonational tunes. Speakers reproduced what they heard on a new sentence such as "They honored Melanie", effectively transposing the tune. 

We are interested on the intonational tune in the final stress-initial word, like "Marilyn" which was modeled on 8 different resynthesized f0 contours - all combinations of H and L mono-tonal pitch accents, phrase accents, and boundary tones in MAE ToBI [@beckman1997guidelines]. We call this the "Nuclear Tune". The model tunes, with 5 schematic target heights are shown below, as well as the data we'll use in this tutorial, which comes from 5 speakers.

```{r,echo=FALSE,fig.height = 4, fig.width = 9.5}
plot_grid(
ggplot(targets,aes(x=t2,y=height,color=tune))+
  geom_line(alpha=1,linetype="solid",size=1.5)+
  geom_point(color="black",size=1.5)+
  ggtitle("Model tunes")+
  xlab("model tune with points aligned as e.g., 'Ma-ri-lyn'")+ylab("target height")+
  theme(legend.position="none",axis.text.x = element_blank(),
  axis.ticks.x = element_blank())+facet_wrap(~tune,nrow=1),

ggplot(data,aes(x=prop_dur,y=erbc,group=speaker_trial,color=tune))+
  stat_summary(fun=mean,geom="line",lwd=2,lty=1,alpha=0.45,aes(group=paste0(speaker,tune)))+
  stat_summary(fun=mean,geom="line",size=1,aes(group=tune),color="black",lty=2)+
  theme(axis.text.x=element_blank())+
  ggtitle("Speaker means (n=5)")+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune,nrow =  1)+theme(legend.position = "none"),
align = "hv",
nrow=2)

```

We measured f0 at 30-time normalized samples across each trisyllabic stress-initial word, and we are interested in examining how f0 varies as a function of which tune participants are imitating. The contours above show average f0 contours for each of 5 speakers per tune, plus the overall mean in each panel. 

Note the data includes multiple repetitions of each tune (up to 18 per speaker), and we excluded data which contained likely f0 tracking errors. We'll model f0 measured as speaker-mean-centered ERB. 

* There are a total of 663 productions (with 30 f0 samples per production)
* The data frame itself is in "long" format, as in:

```{r}
tibble(data)
```

## Issues with modeling intonation

Let's say we want to understand how f0 changes over the course of (normalized) time. Let's focus on two tunes, HLH and LHH, to illustrate. To keep things simple let's just model the mean contour for each tune. This data has the same structure as the full data set, but is much simplified.

```{r} 
tibble(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",])
```

```{r,echo=FALSE,message=FALSE,fig.height = 2.5, fig.width = 7}

 ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  #stat_summary(fun=mean,geom="line",alpha=0.4,size=1)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=3)+
  #geom_smooth(color="blue",method="lm",se=F)+
  #theme(axis.text.x=element_blank())+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune)+theme(legend.position = "none")
```

We *could* capture the relationship between f0 and time with a linear equation as in: 
$$y_{f0} = \alpha + \beta_{1} x_{normtime}$$
If we fit a linear regression to each mean trajectory, we get the following: 

```{r,echo=T,message=FALSE,fig.height = 3, fig.width = 7}
HLH_lm<-lm(erbc_mean~prop_dur,data=data_means[data_means$tune=="HLH",])
LHH_lm<-lm(erbc_mean~prop_dur,data=data_means[data_means$tune=="LHH",])
```

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 7}

ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  #stat_summary(fun=mean,geom="line",alpha=0.4,size=1)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=3)+
  geom_smooth(color="blue",method="lm",se=F)+
  #theme(axis.text.x=element_blank())+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune)+theme(legend.position = "none")
```

This is not a great way to model the data for several reasons: 

* Conceptually, the non-linearities of the f0 movement are simply not captured by a linear model 
  + Especially for the HLH tune, modeling it as simply declining in f0 is missing a lot of what is going on
  
* More practically, they way a linear model is fit to trajectories such as these is also probelmatic, particularly in the way it impacts the model residuals, plotted below (also with auto-correlation functions):

```{r,echo=FALSE,message=FALSE,fig.height = 4, fig.width = 7}

acf1<-ggAcf(
  HLH_lm$residuals,
  type = c("correlation"),
  plot = TRUE)

  
acf2<-ggAcf(
  LHH_lm$residuals,
  type = c("correlation"),
 plot = TRUE)



plot_grid(
  
  ggplot(HLH_lm,aes(y=HLH_lm$residuals,x=c(1:30/30)))+
        geom_hline(yintercept = 0,lty=2,color="gray70")+
         geom_point(pch=4,size=2)+
        ggtitle("Residuals for HLH")+
        ylim(-1.2,1.2)+
        ylab("residual")+
        xlab("prop. word duration"),

      ggplot(LHH_lm,aes(y=LHH_lm$residuals,x=c(1:30/30)))+
          geom_hline(yintercept = 0,lty=2,color="gray70")+
          geom_point(pch=4,size=2)+
          ggtitle("Residuals for LHH")+
          ylim(-1.2,1.2)+
          ylab("")+
           xlab("prop. word duration"),
  
  
acf1+ggtitle(""),
acf2+ggtitle(""),nrow = 2,rel_heights = c(1,1))

  
```


As we can see, the residuals from each fit shows a systematic patterning (we can imagine with more trajetories this same issue would hold) - this is bad news for making statistical inferences, since residuals should not be correlated with one another in linear regressions.

GAM(M)s offer an attractive alternative for capturing non-linear, and particularly time-series data (though we also need to consider autocorrelation in GAM(M)s, discussed below). 

* There are of course alternatives to GAM(M)s that also work for non-linear data, for example fitting various polynomial functions, but these also have shortcomings. 

+ <span style="color: purple;">Further reading: @gamsite and @winter2016analyze for motivating GAM(M)s and the shortcomings of other alternatives.</span>

GAM(M)s have been applied to diferent sorts of linguistic research including eye movement data [@zahner2019alignment], formants in a vowel [@stuart2015dynamic], and diachronic change [@fruehwald2016early].  

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 7}
#plot_grid(
#acf1+ggtitle("HLH"),
#acf2+ggtitle("LHH"),nrow = 1)
```


## GAM basics

Let's start by fitting a GAM  to the simple HLH trajectory, using the package ```mgcv``` Note this is not a GAMM since we don't have random effects. 

```{r,echo=TRUE,message=FALSE,fig.height = 3, fig.width = 7}
library(mgcv)
gam_tp_10<-bam(erbc_mean~s(prop_dur,bs="tp",k=10),
          data=data_means[data_means$tune=="HLH",])
```
When we plot the GAM fit we get: 

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 5}

tp_10_fit<-as.data.frame(cbind(gam_tp_10$fitted.values,1:30/30))
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
   stat_summary(fun=mean,geom="point",color="black",pch=1,size=5)+
      ylim(-1.5,1.5)+
    geom_line(data=tp_10_fit,aes(y=V1,x=V2),color="blue",size=2)+
  xlab("prop. word duration")+ylab("centered ERB")+theme(legend.position = "none")
```


Looks pretty good! Much better than a linear approximation, but what is actually being fit?...

The GAM is predicting  speaker-centered ERB (erbc_mean) as a function of a *smooth* term for normalized time (prop_dur) ``` ~s(prop_dur,bs="tp",k=10)```. Two components in the model terms are: ``` bs="tp" ``` and ```k=10```

### 1. Basis functions

Let's first focus on the former: ``` bs```  stands for basis function- which defines a set of functions that are fit with the GAM, and summed to create a potentially more complex curve. We'll walk through this below. 

Basis functions come in many different varieties. In this case ``` bs="tp"``` specifies "thin plate regression splines", a particular type of spline which this case is made up of functions of varying complexity and shape. 

Let's take a look: we can extract this information from a GAM(M) fit as follows (see the R markdown document for a full set of code plotting from this object)

```{r,echo=TRUE,message=FALSE,fig.height = 3, fig.width = 9.}
model_matrix_tp_10 <- as.data.frame(predict(gam_tp_10, type = "lpmatrix"))
```


```{r,echo=FALSE,message=FALSE,fig.height = 5, fig.width = 7}

colnames(model_matrix_tp_10)<-c("j","a","b","c","d","e","f","g","h","i")
model_matrix_tp_10%>%gather()->model_matrix_tp_10
model_matrix_tp_10$normtime<-1:30/30

coeff_tp_10<-as.data.frame(gam_tp_10$coefficients)
coeff_tp_10$key<-c("j","a","b","c","d","e","f","g","h","i")
left_join(model_matrix_tp_10,coeff_tp_10)->model_matrix_tp_10

colnames(model_matrix_tp_10)<-c("index", "basis","prop_dur","coeff")

model_matrix_tp_10 %>% mutate(coeff_basis=basis*coeff)->model_matrix_tp_10


ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_10[model_matrix_tp_10$index!="j",],aes(x=prop_dur,y=basis,color=index),size=1)+facet_wrap(~index)+
  ylab("")+
  xlab("")+
  theme(legend.position = "none")
       # axis.text.x = element_blank(),axis.text.y = element_blank())

```

So, how do we get from these various functions to the model fit? The mathematical representation of GAMs is really flexible, it's often just written as: 

$$y_{f0} = \alpha + f(x_{normtime}) + \epsilon$$

Where $f(x)$ is just "some function of x" (and $\epsilon$ is a term for error). 

What $f(x)$ is depends on the number and type of basis functions, like those we see above. A GAM is fit by taking each of these *basis functions* of x, multiplying each function by a coefficient $\beta$ which is computed by the model, and summing the result, as in:

$$ f(x) = \sum_{i=1}^{d} \gamma_{i}(x) \beta_{i} $$
Where $\gamma$ is a basis function, and $d$ is total number of these functions. 

To make this concrete, let's walk through an example. With the model we just fit, we have 9 basis functions, so the model when fit produces 9 coefficients, in addition to the intercept. 

```{r,echo=TRUE,message=FALSE,fig.height = 4, fig.width = 6}

gam_tp_10$coefficients

```

By multiplying each coefficient by its respective basis function we arrive at the following:


```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 9.5}
plot_grid(
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_10[model_matrix_tp_10$index!="j",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
   ylim(-2.1,2.1)+
  ggtitle("basis functions")+
  theme(legend.position = "none"),


ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_10[model_matrix_tp_10$index!="j",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
   ylim(-2.1,2.1)+
  ylab("")+xlab("")+
  ggtitle("multiplied by βs")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_10[model_matrix_tp_10$index!="j",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
  geom_hline(yintercept =-0.33915039,size=1,lty=2)+
  ylab("")+xlab("")+
  theme(legend.position = "none")+
     ylim(-2.1,2.1)+
   ggtitle("plus the intercept")+
  stat_summary(fun=mean,geom="point",color="black",pch=1,size=5),
  align="hv",nrow=1,labels="AUTO")

```

These plots are coming from a data frame in which I extracted the basis functions and intercept from the model, and then multiplied by the the coefficients, for example for basis function "a":

```{r,echo=F}
model_matrix_tp_10 %>% arrange(index) ->model_matrix_tp_10
```

```{r,echo=T}
tibble(model_matrix_tp_10)
```

So, let's just sum up the basis functions and create a data frame with that summed fit

* This will be the sum of all the lines (basis functions) in panel C above and the intercept


```{r,echo=T}
model_matrix_tp_10 %>% group_by(prop_dur)%>% # group by time
  mutate(fit= sum(coeff_basis)) %>% select(fit,prop_dur) %>% # sum coeffs 
  slice(1)->reconstructed_fit_tp # get just one observeration per time stamp

print(as_tibble(reconstructed_fit_tp,n=30))
```


```{r,echo=TRUE,fig.height = 3, fig.width = 5}

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
   stat_summary(fun=mean,geom="point",color="black",pch=1,size=5)+
    geom_line(data=reconstructed_fit_tp,aes(y=fit,x=prop_dur),color="orchid2",size=2)+ # the fit we just made from the basis functions
      ylim(-1.5,1.5)+
  xlab("prop. word duration")+ylab("centered ERB")+theme(legend.position = "none")


```

We can see that by summing the lines in panel C and intercept, we have thus arrived at the model's fit to the data. 

A GAM can be fit with many types of basis functions, for comparison let's fit a GAM with ``` bs="cr" ``` for "cubic regression" splines:


```{r,echo=TRUE,message=FALSE,fig.height = 3, fig.width = 7}
gam_cr_10<-bam(erbc_mean~s(prop_dur,bs="cr",k=10),
          data=data_means[data_means$tune=="HLH",])
```

Like before, let's take a look at the basis functions before and after they are multiplied by the model coefficients: 


```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 7}
model_matrix_cr_10 <- as.data.frame(predict(gam_cr_10, type = "lpmatrix"))

colnames(model_matrix_cr_10)<-c("j","a","b","c","d","e","f","g","h","i")
model_matrix_cr_10%>%gather()->model_matrix_cr_10
model_matrix_cr_10$normtime<-1:30/30

coeff_cr_10<-as.data.frame(gam_cr_10$coefficients)
coeff_cr_10$key<-c("j","a","b","c","d","e","f","g","h","i")
left_join(model_matrix_cr_10,coeff_cr_10)->model_matrix_cr_10

colnames(model_matrix_cr_10)<-c("index", "basis","prop_dur","coeff")
model_matrix_cr_10 %>% mutate(coeff_basis=basis*coeff)->model_matrix_cr_10

model_matrix_cr_10 %>% group_by(prop_dur)%>% # group by time
  mutate(fit= sum(coeff_basis)) %>% select(fit,prop_dur) %>% 
  slice(1)->reconstructed_fit_cr 
```

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 9.5}
plot_grid(
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_10[model_matrix_cr_10$index!="j",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
   ylim(-1.5,1.5)+
  ggtitle("basis functions")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_10[model_matrix_cr_10$index!="j",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
ylim(-1.5,1.5)+
  ylab("")+xlab("")+
  ggtitle("multiplied by coefficients")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_10[model_matrix_cr_10$index!="j",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
  geom_hline(yintercept =-0.33915039,size=1,lty=2)+
  ylab("")+xlab("")+
  theme(legend.position = "none")+
   ylim(-1.5,1.5)+
   ggtitle("plus the intercept")+
  stat_summary(fun=mean,geom="point",color="black",pch=1,size=5),
  align="hv",nrow=1,labels="AUTO")

```
The cubic regression splines look really different as compared to the thin plate regression splines, but at the end of the day they produce essentially the same fit as shown below:

```{r,echo=FALSE,fig.height = 3, fig.width = 8}
plot_grid(
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
   stat_summary(fun=mean,geom="point",color="black",pch=1,size=5)+
  ggtitle("bs=cr")+
    #geom_line(data=reconstructed_fit_tp,aes(y=fit,x=prop_dur),color="orchid2",size=2)+ # the fit we just made from the basis functions
      ylim(-1.5,1.5)+
   geom_line(data=reconstructed_fit_cr,aes(y=fit,x=prop_dur),color="cyan3",size=2)+ # the fit we just made from the basis functions
  xlab("prop. word duration")+ylab("centered ERB")+theme(legend.position = "none"),


ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
   stat_summary(fun=mean,geom="point",color="black",pch=1,size=5)+
    ggtitle("bs=tp")+
    geom_line(data=reconstructed_fit_tp,aes(y=fit,x=prop_dur),color="orchid2",size=2)+ # the fit we just made from the basis functions
     ylim(-1.5,1.5)+
   #geom_line(data=reconstructed_fit_cr,aes(y=fit,x=prop_dur),color="blue",size=2,lty=2)+ # the fit we just made from the basis functions
  xlab("prop. word duration")+ylab("")+theme(legend.position = "none"),nrow=1)

```

In my experience, the basis functions one can employ in ```mgcv``` don't drastically change the fit or inferences about the data. The default in the package is  ``` bs=tp```.


### 2. Knots 

Now that we've covered the basis functions, let's turn to the other parameter we set when fitting the models, namely ```k=10```. k stands for "knots", which determines the number of basis functions we employ (9 in the examples above). Now let's look at ```k=6```. 

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 6}

gam_cr_6<-bam(erbc_mean~s(prop_dur,bs="cr",k=6),
          data=data_means[data_means$tune=="HLH",])
model_matrix_cr_6 <- as.data.frame(predict(gam_cr_6, type = "lpmatrix"))

colnames(model_matrix_cr_6)<-c("f","a","b","c","d","e")
model_matrix_cr_6%>%gather()->model_matrix_cr_6
model_matrix_cr_6$normtime<-1:30/30


ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_6[model_matrix_cr_6$key!="f",],aes(x=normtime,y=value,color=key),size=1)+ylab("")+xlab("")+
   ylim(-1,1)+
  ggtitle("bs = cr, k= 6")+
  theme(legend.position = "none")
```


For cubic regression splines, knots define piece-wise cubic  functions that contribute to the overall spline, and are more intuitively visible as places where splines converge. For thin plate splines, things are more complicated, and for our purposes we can think of  ```k``` as just determining the *number of basis functions* $d$, where $d = k-1$. So when specifying ```k=10``` above, we used 9 basis functions. 

* <span style="color: purple;"> Further reading: @wood2003thin for a math-heavy introduction to thin plate regression splines and discussion of knot construction. </span>

Let's look at two types of basis functions, with different numbers of knots, fit to the HLH trajectory. The labels for each row indicate both the basis funciton cr/tp and the knot number. 



```{r,message=FALSE}
gam_cr_5<-bam(erbc_mean~s(prop_dur,bs="cr",k=5), #repeated from above
          data=data_means[data_means$tune=="HLH",])
model_matrix_cr_5 <- as.data.frame(predict(gam_cr_5, type = "lpmatrix"))

colnames(model_matrix_cr_5)<-c("e","a","b","c","d")
model_matrix_cr_5%>%gather()->model_matrix_cr_5
model_matrix_cr_5$normtime<-1:30/30

coeff_cr_5<-as.data.frame(gam_cr_5$coefficients)
coeff_cr_5$key<-c("e","a","b","c","d")
left_join(model_matrix_cr_5,coeff_cr_5)->model_matrix_cr_5

colnames(model_matrix_cr_5)<-c("index", "basis","prop_dur","coeff")
model_matrix_cr_5 %>% mutate(coeff_basis=basis*coeff)->model_matrix_cr_5

model_matrix_cr_5 %>% group_by(prop_dur)%>% #
  mutate(fit= sum(coeff_basis)) %>% select(fit,prop_dur) %>% 
  slice(1)->reconstructed_fit_cr_5


gam_cr_3<-bam(erbc_mean~s(prop_dur,bs="cr",k=3),
          data=data_means[data_means$tune=="HLH",])
model_matrix_cr_3 <- as.data.frame(predict(gam_cr_3, type = "lpmatrix"))

colnames(model_matrix_cr_3)<-c("c","a","b")
model_matrix_cr_3%>%gather()->model_matrix_cr_3
model_matrix_cr_3$normtime<-1:30/30

coeff_cr_3<-as.data.frame(gam_cr_3$coefficients)
coeff_cr_3$key<-c("c","a","b")
left_join(model_matrix_cr_3,coeff_cr_3)->model_matrix_cr_3

colnames(model_matrix_cr_3)<-c("index", "basis","prop_dur","coeff")
model_matrix_cr_3 %>% mutate(coeff_basis=basis*coeff)->model_matrix_cr_3

model_matrix_cr_3 %>% group_by(prop_dur)%>% #
  mutate(fit= sum(coeff_basis)) %>% select(fit,prop_dur) %>% 
  slice(1)->reconstructed_fit_cr_3


gam_tp_5<-bam(erbc_mean~s(prop_dur,bs="tp",k=5),
          data=data_means[data_means$tune=="HLH",])
model_matrix_tp_5 <- as.data.frame(predict(gam_tp_5, type = "lpmatrix"))

colnames(model_matrix_tp_5)<-c("e","a","b","c","d")
model_matrix_tp_5%>%gather()->model_matrix_tp_5
model_matrix_tp_5$normtime<-1:30/30

coeff_tp_5<-as.data.frame(gam_tp_5$coefficients)
coeff_tp_5$key<-c("e","a","b","c","d")
left_join(model_matrix_tp_5,coeff_tp_5)->model_matrix_tp_5

colnames(model_matrix_tp_5)<-c("index", "basis","prop_dur","coeff")
model_matrix_tp_5 %>% mutate(coeff_basis=basis*coeff)->model_matrix_tp_5

model_matrix_tp_5 %>% group_by(prop_dur)%>% #
  mutate(fit= sum(coeff_basis)) %>% select(fit,prop_dur) %>% 
  slice(1)->reconstructed_fit_tp_5


gam_tp_3<-bam(erbc_mean~s(prop_dur,bs="tp",k=3),
          data=data_means[data_means$tune=="HLH",])
model_matrix_tp_3 <- as.data.frame(predict(gam_tp_3, type = "lpmatrix"))

colnames(model_matrix_tp_3)<-c("c","a","b")
model_matrix_tp_3%>%gather()->model_matrix_tp_3
model_matrix_tp_3$normtime<-1:30/30

coeff_tp_3<-as.data.frame(gam_tp_3$coefficients)
coeff_tp_3$key<-c("c","a","b")
left_join(model_matrix_tp_3,coeff_tp_3)->model_matrix_tp_3

colnames(model_matrix_tp_3)<-c("index", "basis","prop_dur","coeff")
model_matrix_tp_3 %>% mutate(coeff_basis=basis*coeff)->model_matrix_tp_3

model_matrix_tp_3 %>% group_by(prop_dur)%>% #
  mutate(fit= sum(coeff_basis)) %>% select(fit,prop_dur) %>% 
  slice(1)->reconstructed_fit_tp_3


```

```{r,echo=FALSE,message=FALSE,fig.height = 9.5, fig.width = 9.5}
plot_grid(
 plot_grid(
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_5[model_matrix_cr_5$index!="e",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
   ylim(-2,2)+
  ggtitle("bs = cr, k = 5")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_5[model_matrix_cr_5$index!="e",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
  ylim(-2,2)+
  ylab("")+xlab("")+
  ggtitle("multiplied by βs")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_5[model_matrix_cr_5$index!="e",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
    geom_line(data=reconstructed_fit_cr_5,aes(y=fit,x=prop_dur),color="blue",size=2)+
  geom_hline(yintercept =-0.33915039,size=1,lty=2)+
  ylab("")+xlab("")+
  theme(legend.position = "none")+
    ylim(-2,2)+
   ggtitle("plus the intercept and fit ")+
  stat_summary(fun=mean,geom="point",color="black",pch=1,size=3),

nrow=1),

plot_grid(
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_5[model_matrix_tp_5$index!="e",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
   ylim(-2.7,2.7)+
  ggtitle("bs = tp, k = 5")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_5[model_matrix_tp_5$index!="e",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
  ylim(-2.7,2.7)+
  ylab("")+xlab("")+
  ggtitle("")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_5[model_matrix_tp_5$index!="e",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
    geom_line(data=reconstructed_fit_tp_5,aes(y=fit,x=prop_dur),color="blue",size=2)+
  geom_hline(yintercept =-0.33915039,size=1,lty=2)+
  ylab("")+xlab("")+
  theme(legend.position = "none")+
    ylim(-2.7,2.7)+
   ggtitle("")+
  stat_summary(fun=mean,geom="point",color="black",pch=1,size=3),

nrow=1),


plot_grid(
ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_3[model_matrix_cr_3$index!="c",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
  ylim(-2,2)+
  ggtitle("bs = cr, k = 3")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_3[model_matrix_cr_3$index!="c",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
   ylim(-2,2)+
  ylab("")+xlab("")+
  ggtitle("")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_cr_3[model_matrix_cr_3$index!="c",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
    geom_line(data=reconstructed_fit_cr_3,aes(y=fit,x=prop_dur),color="blue",size=2)+
  geom_hline(yintercept =-0.33915039,size=1,lty=2)+
  ylab("")+xlab("")+
  theme(legend.position = "none")+
    ylim(-2,2)+
   ggtitle("")+
  stat_summary(fun=mean,geom="point",color="black",pch=1,size=3),

nrow=1),

plot_grid(

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_3[model_matrix_tp_3$index!="c",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
  ylim(-2,2)+
  ggtitle("bs = tp, k = 3")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_3[model_matrix_tp_3$index!="c",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
   ylim(-2,2)+
  ylab("")+xlab("")+
  ggtitle("")+
  theme(legend.position = "none"),

ggplot(data_means[data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_3[model_matrix_tp_3$index!="c",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
    geom_line(data=reconstructed_fit_tp_3,aes(y=fit,x=prop_dur),color="blue",size=2)+

  geom_hline(yintercept =-0.33915039,size=1,lty=2)+
  ylab("")+xlab("")+
  theme(legend.position = "none")+
    ylim(-2,2)+
   ggtitle("")+
  stat_summary(fun=mean,geom="point",color="black",pch=1,size=3),
nrow=1),
  align="hv",nrow=4,labels="AUTO")

```


As is clear from above, the more knots (and basis functions), the "wigglier" a curve can be, allowing us to capture more complicated trajectory shapes.


Another important determinant of the GAM's fit is the *smoothing parameter*, which is estimated to optimize for over- and under-fitting the data. In ```mgcv``` this parameter is estimated directly from the data, such that you as the modeler don't have to set it.

* <span style="color: purple;"> Further reading: @soskuthy2017generalised, @gamsite and @wood2017generalized for more information on smoothing parameter estimation.</span>

Because of the smoothing parameter, the number of knots imposes an upper limit on the wiggliness of a trajectory, but the actual fit will not necessarily increase in wiggliness once an adequate number of knots has been exceeded. 

We can see this for ```tp``` fits below: note that going from 10 to 30 knots doesn't really change anything. 

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 10}

plot_grid(
ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  ggtitle("k = 3")+
  ylim(-2,2)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=2)+
  geom_smooth(color="blue",method="gam",se=F,formula = y ~ s(x, bs="tp",k = 3))+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune,nrow=2)+theme(axis.title.x=element_blank(),axis.text.x=element_blank(),legend.position = "none"),
  
ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  ggtitle("k = 4")+
    ylim(-2,2)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=2)+
  geom_smooth(color="blue",method="gam",se=F,formula = y ~ s(x, bs="tp",k = 4))+
  theme(axis.title.y=element_blank(),axis.title.x=element_blank(),axis.text.x=element_blank(),legend.position = "none")+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune,nrow=2),


ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  ggtitle("k = 5")+
  ylim(-2,2)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=2)+
  geom_smooth(color="blue",method="gam",se=F,formula = y ~ s(x,bs="tp", k = 5))+
  theme(axis.title.y=element_blank(),axis.title.x=element_blank(),axis.text.x=element_blank(),legend.position = "none")+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune,nrow=2),

ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  ggtitle("k = 10")+
     ylim(-2,2)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=2)+
  geom_smooth(color="blue",method="gam",se=F,formula = y ~ s(x,bs="tp", k = 10))+
  theme(axis.title.y=element_blank(),axis.title.x=element_blank(),axis.text.x=element_blank(),legend.position = "none")+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune,nrow=2),

ggplot(data_means[data_means$tune=="LHH"|data_means$tune=="HLH",],aes(x=prop_dur,y=erbc_mean,color=tune))+
  ggtitle("k = 30")+
  ylim(-2,2)+
  stat_summary(fun=mean,geom="point",aes(group=tune),color="black",pch=1,size=2)+
  geom_smooth(color="blue",method="gam",se=F,formula = y ~ s(x, k = 30))+
  theme(axis.title.y=element_blank(),axis.title.x=element_blank(),axis.text.x=element_blank(),legend.position = "none")+
  xlab("prop. word duration")+ylab("centered ERB")+facet_wrap(~tune,nrow=2), align="hv",nrow=1)
```

In practice, setting a very high number of knots is not necessarily a good idea because it is computationally intensive, and can sometimes lead to over-fitting. 

There is no hard and fast rule for choosing the right number of knots, and the number you select should be informed by how "wiggly" you expect your data to be. 

 In my experience 10 knots is a good amount for modeling trajectories of the sort we see in intonational tunes over a multi-syllable word. The package ```mgcv``` comes with useful ways to check that the number of knots you are setting is appropriate.

* This can be done with the ```gam.check()``` and ```check.k()``` functions in ```mgcv```. 

* <span style="color: purple;"> Further reading: see @soskuthy2017generalised  for tips on choosing an appropriate number of knots for your data. </span>


### 3. Scaling things up

In the preceding toy examples, we are fitting a single trajectory, that is, one single time series measurement of f0. However, everything we just saw applies when we scale up to modeling, say, all of the LHH contours in the data set, as in:

```{r,echo =F, fig.height = 3.5, fig.width = 5}
ggplot(data[data$tune=="LHH",],aes(x=prop_dur,group=speaker_trial,y=erbc))+
    geom_line(alpha=0.2,size=1.5)+ # the fit we just made from the basis functions
  xlab("prop. word duration")+ylab("centered ERB")+theme(legend.position = "none")+
  ggtitle("LHH tunes")
```

```{r,echo=TRUE,message=FALSE,fig.height = 3, fig.width = 7}
gam_tp_10_LHH<-bam(erbc~s(prop_dur,bs="tp",k=10),
          data=data[data$tune=="LHH",])
```


Let's look at the fit in the same way as with the single-trjacetory models: here you can see the same principles we examined for a single trajectory apply to the fit to all of the trajectories here. 

```{r,echo=FALSE,message=FALSE,fig.height = 3, fig.width = 9.5}
tp_10_LHH_fit<-as.data.frame(cbind(gam_tp_10_LHH$fitted.values,1:30/30))


model_matrix_tp_10_LHH <- as.data.frame(predict(gam_tp_10_LHH, type = "lpmatrix"))

colnames(model_matrix_tp_10_LHH)<-c("j","a","b","c","d","e","f","g","h","i")
model_matrix_tp_10_LHH%>%gather()->model_matrix_tp_10_LHH
model_matrix_tp_10_LHH$normtime<-1:30/30

coeff_tp_10_LHH<-as.data.frame(gam_tp_10_LHH$coefficients)
coeff_tp_10_LHH$key<-c("j","a","b","c","d","e","f","g","h","i")
left_join(model_matrix_tp_10_LHH,coeff_tp_10_LHH)->model_matrix_tp_10_LHH

colnames(model_matrix_tp_10_LHH)<-c("index", "basis","prop_dur","coeff")
model_matrix_tp_10_LHH %>% mutate(coeff_basis=basis*coeff)->model_matrix_tp_10_LHH

#ggplot(data[data$tune=="LHH",],aes(x=prop_dur,y=erbc))+
 #geom_line(alpha=0.1,size=0.8,aes(group=speaker_trial))+ 
  #geom_line(data=tp_10_LHH_fit,aes(y=V1,x=V2),color="blue",size=2)+
  #xlab("prop. word duration")+ylab("centered ERB")+theme(legend.position = "none")


plot_grid(
ggplot(data[data$tune=="LHH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_10_LHH[model_matrix_tp_10_LHH$index!="j",],aes(x=prop_dur,y=basis,color=index),size=1)+ylab("")+xlab("")+
  ylim(-2,2)+
  ggtitle("basis functions")+
  theme(legend.position = "none"),


ggplot(data[data$tune=="LHH",],aes(x=prop_dur,y=erbc_mean))+
  geom_line(data=model_matrix_tp_10_LHH[model_matrix_tp_10_LHH$index!="j",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
    ylim(-2,2)+
  ylab("")+xlab("")+
  ggtitle("multiplied by βs")+
  theme(legend.position = "none"),

ggplot(data[data$tune=="LHH",],aes(x=prop_dur,y=erbc))+
    geom_line(alpha=0.1,size=1.5,aes(group=speaker_trial))+ 
  geom_line(data=model_matrix_tp_10_LHH[model_matrix_tp_10_LHH$index!="j",],aes(x=prop_dur,y=coeff_basis,color=index),size=1)+
  geom_hline(yintercept =0.0779,size=1,lty=2)+
  coord_cartesian(ylim=c(-2,2))+
  ylab("")+xlab("")+
   geom_line(data=tp_10_LHH_fit,aes(y=V1,x=V2),color="blue",size=3)+
  theme(legend.position = "none")+
   ggtitle("plus the intercept and fit"),
  align="hv",nrow=1,labels="AUTO")

```


## Comparing tunes, random effects, and error models

So far we've just fit to a single tune's trajectory, allowing us to characterize its shape. However, usually, we want to characterize how tunes differ from one another. 

The remainder of this tutorial will walk through this, comparing just two tunes: HHH and HHL, the "high rising" tunes. 


```{r,echo =T}

data %>% filter(tune=="HHH"|tune=="HHL") -> HR

```
Here you can see all of the data with means for each tune
```{r,echo =F,fig.height = 3, fig.width = 7}

ggplot(HR,aes(x=prop_dur,y=erbc,group=speaker_trial,color=tune))+
  geom_line(alpha=0.2,size=1.5)+facet_wrap(~tune)+
  xlab("prop. word duration")+ylab("centered ERB")+
  theme(legend.position = "none")+
  stat_summary(fun=mean,geom="line",aes(group=tune),lty=2,size=1.5,color="blue")

```

### 1. Comparing tune shapes

To begin let's fit a simple model that includes each tune. You'll see that the model code is a bit different.

Note that in this model we are fitting separate smooths to each trajectory, and then inspecting how these smooths vary from one another. 

* An alternative, not employed here, is to explicitly fit a trajectory that captures the difference between smooths. The model fit is the same, however when the difference is fit, the model coefficients become more readily interpretable (we'll see that they are pretty opaque below). 

* See @soskuthy2017generalised for discussion on this, and see the RMarkdown code for an implementation (not shown in the html document). 


```{r,echo=TRUE,message=FALSE,fig.height = 3, fig.width = 7}

HR$tune<-as.factor(HR$tune) # need to make tune a factor first
gam_HR<- bam(erbc ~ tune +
             s(prop_dur, by=tune, bs="tp",k=10),
             data=HR)

```

```{r,echo=FALSE,eval=FALSE,message=FALSE,fig.height = 3, fig.width = 7}
# For implementing a model with a specified difference smooth... see Sóskuthy (2017) for more on these.
HR$tune.ordered<-as.ordered(HR$tune) #create ordered factor
contrasts(HR$tune.ordered)<- "contr.treatment" #contrast code
gam_HR.diff.smooth<- bam(erbc ~ tune.ordered +
             s(prop_dur, bs="tp",k=10)+  # reference smooth (HHH)
             s(prop_dur, by=tune.ordered, bs="tp",k=10), #difference smooth 
             data=HR)
```


You can see that "tune" is showing up in two places, once outside the smooth term ``` s(normtime, by=tune, bs="tp",k=10)``` and once inside it. The first instance of tune is a *parametric* term, which is analogous to how we would fit a linear model. This captures overall difference in the height of each trajectory. 

The second, ``` s(normtime, by=tune, bs="tp",k=10)``` is a *smooth term*, fitting separate smooths for each tune. 

Let's look at how each of these comes out in the model summary: 
```{r}
summary(gam_HR)
```

The Parametric coefficients portion of the model summary will look familiar if you've seen linear model outputs in R. We have an estimated intercept (for the reference level HHH), and a parametric term for HHL compared to the reference level. 

The model estimates for the smooth terms, are not particularly useful for interpreting the model, they tell us: 

* "estimated degrees of freedom" and reference degrees of freedom which vary based on the number of basis functions, and the smoothing parameter see @soskuthy2017generalised for more on this.

* F and p values which are testing the null hypothesis that a smooth term = 0 (i.e. a straight line at 0). We can see that the model is quite sure that we can reject his null hypothesis. 
    + Note that if we had a fit a trajectory modeling the *difference* between smooths these coefficients would be much more informative. 

One more intuitive way of inspecting GAM(M)s is to visualize smooths, this can be done easily using the package ```itsadug```, the output of which is shown below. 

```{r,fig.height = 4, fig.width = 5,message=F,results=F}
library(itsadug)
plot_smooth(gam_HR, view = "prop_dur", plot_all = "tune",rm.ranef=T)
```

Because each smooth fit also includes CI, we can make some principled comparisons between trajectories. 
   
However before we do this, we are missing one crucial component: random effects. 

### 2. Adding Random effects

There are various ways of specifying random effect structures in GAMMs. Here I will present just one, which is discussed in @soskuthy2021evaluating: so-called *random smooths* which are constructed to be analogous to random intercepts for speaker and slopes for tune in a linear model. 

We'll add two lines to our model code:

```{r, echo=TRUE,eval = FALSE}
  s(prop_dur,speaker,bs="fs",m=1,k=10)+
  s(prop_dur,speaker, by = tune.ordered, bs="fs",m=1,k=10) 
```
You can see that we've added a reference smooth by time for speaker, with the basis function "fs" which stands for "factor smooth interaction", and is necessary for specifying random smooths. We also have a new setting in each line ``` m=1 ```, which adjusts the smoothing penalty applied in these smooths (from the default 2) which is recommended in a couple of places. 

* Note that tune must be recoded as an orderd variable

* <span style="color: purple;"> Further reading: see @soskuthy2017generalised and for in depth discussion of different ways to fit random effects in GAMMs. </span>

* <span style="color: purple;"> Further reading: see @soskuthy2017generalised and @baayen2018autocorrelated for more on smoothing penalties. </span>

The full model now looks like:

```{r,echo=TRUE}
HR$speaker<-as.factor(HR$speaker)
HR$tune.ordered<-as.ordered(HR$tune) # tune needs to become an ordered factor for this to work
contrasts(HR$tune.ordered)<- "contr.treatment" # contrast code - this is just how these types of smooths are coded in mgcv
gam_HR_random<- bam(erbc ~ tune +
             s(prop_dur, by=tune, bs="tp",k=10)+
             s(prop_dur,speaker,bs="fs",m=1,k=10)+
             s(prop_dur,speaker, by = tune.ordered, bs="fs",m=1,k=10),
             data=HR)
```


```{r,fig.height = 4, fig.width = 5,message=F,results=F}
plot_smooth(gam_HR_random, view = "prop_dur", plot_all = "tune",rm.ranef=T)
```

Here we can see that the addition of random smooths has added more uncertainty around the estimates. 

* <span style="color: purple;"> Further reading: @soskuthy2017generalised and  @soskuthy2021evaluating for descriptions of random effects in GAMMs and random effects more generally.</span>
  
### 3. Adding an AR1 error model

One potential issue that GAMMs can have is, like linear models, auto-correlated residuals.

To see if residuals are indeed correlated, we can inspect this using another function from ```itsadug```: 

```{r, echo=TRUE,fig.height = 4, fig.width = 5}
acf_resid(gam_HR_random)
```

There is indeed considerable auto-correlation. One recommended solution for dealing with this is to use a so-called AR1 error model. 

An AR1 model is a specific type of auto-regressive error model, which means a model that assumes correlation when estimating parameters, and is recommended for dealing with auto-correlation (the 1 in AR1 means that we are just assuming correlation between immediately adjacent points in the time-series, it is used a lot in GAMM modeling, partly because it is the type of error model that is integrated with ```mgcv```)

* <span style="color: purple;"> Further reading: see @soskuthy2017generalised and @baayen2018autocorrelated for more on autrogressive error models, and discussion of the usefullness of AR1 models for GAMMs.</span>

Setting up an AR1 model entails a couple of things: 

* Ordering rows in the data frame to correspond to the time series 
* Telling the model at what point in time each trajectory begins, this can be set as a ``` TRUE/FALSE ``` variable that is ```TRUE``` at the start of each trajectory. 
* Providing an estimate of the correlation between errors- this can be extracted from the previous non-AR1 version of the model we ran.  

The code for implmenting this AR-1 model is shown below. 

```{r, echo=TRUE,fig.height = 3, fig.width = 4}

HR$start.event <- HR$normtime==1  # set event starts
rho1<-start_value_rho(gam_HR_random) # estimate correlation from model

gam_HR_random_AR1<- bam(erbc ~ tune +
             s(prop_dur, by=tune, bs="tp",k=10)+
             s(prop_dur,speaker,bs="fs",m=1,k=10)+
             s(prop_dur,speaker, by = tune.ordered, bs="fs",m=1,k=10),
             rho=rho1, AR.start = HR$start.event,
             data=HR)
```

By plotting the residuals like before we can see that the AR1 model does a much better job of dealing with auto-correlated residuals. 

```{r, echo=TRUE,fig.height = 4, fig.width = 5}
acf_resid(gam_HR_random_AR1)
```

Here we can plot the model fit, and see that its quite similar to the non-AR1 variant, though we can feel better knowing we've removed correlation among the residuals. 

```{r, echo=FALSE,fig.height = 4, fig.width = 5,results = FALSE,message=FALSE}
plot_smooth(gam_HR_random_AR1, view = "prop_dur", plot_all = "tune",rm.ranef=T)
```

## Visual inspection for significance

There are various ways to do significance testing in GAMMs, and the appropriate one may vary depending on your question. These include: 

* Inspecting coefficients that model differences explicitly (not possible with the way we fit our model)
* Model comparison 
* Visual inspection of fits, difference smooths and CI. 

Given that here we are interested in characterizing how the shapes of the contours differ from one another, a straightforward test of significance can come from visual inspection of smooth differences: so-called difference smooths. 

* Note that these can be computed by the model even if we didn't specifically hard-code a difference smooth term (the benefit of doing that would just be to have more redily interpretable coefficents). 

* <span style="color: purple;">Further reading: @soskuthy2017generalised and @soskuthy2021evaluating for other significance tests in GAMM models.</span>



This visualization shows the estimated difference between two fits, and the model's confidence in that estimate. When the difference and CI exclude 0, we can say this represents a significant diffference *at this location in (normalized) time* between the two smooths. These plots can be generated easily with ```itsadug```.

```{r,fig.height = 3.5,message=FALSE, fig.width = 5,results = FALSE}
plot_diff(gam_HR_random_AR1, view = "prop_dur",comp = list(tune=c("HHH","HHL")),rm.ranef=T)
```

As a final note, its easy enough to extract model fits and difference smooths for plotting elsewhere, with for example ```ggplot2``` (see the RMarkdown version of this document for code), so we could re-plot the fit and difference smooth like so: 

```{r,echo=FALSE,message=FALSE,fig.show='hide',results = FALSE}

HHH_HHL<-plot_diff(gam_HR_random_AR1, view = "prop_dur",comp = list(tune=c("HHH","HHL")),rm.ranef=T)

HHH_HHL_fit<-plot_smooth(gam_HR_random_AR1, view = "prop_dur", plot_all = "tune",rm.ranef=T)
HHH_HHL_fit<-HHH_HHL_fit$fv
```

```{r,fig.height = 4, fig.width = 8}

HHH_HHL_highlight<-ifelse(HHH_HHL$est+HHH_HHL$CI<0|HHH_HHL$est-HHH_HHL$CI>0,1,0)
HHH_HHL$inds <- diff(c(0, HHH_HHL_highlight))
start <- HHH_HHL$prop_dur[HHH_HHL$inds == 1];start<-start[!is.na(start)]
end <- c(HHH_HHL$prop_dur[HHH_HHL$inds == -1]);end<-end[!is.na(end)]
if (length(start) > length(end)) end <- c(end, tail(HHH_HHL$prop_dur, 1))
rects_HHH_HHL <- data.frame(start=start, end=end, group=seq_along(start))


plot_grid(
ggplot(HHH_HHL_fit,aes(x=prop_dur,y=fit))+ 
  geom_ribbon(aes(ymin=ll,ymax=ul,fill=tune),alpha=0.2)+
    coord_cartesian(ylim=c(-1,2))+
    theme(legend.position = "bottom")+
    ggtitle("Model fit")+
    geom_line(aes(color=tune))+xlab("prop. word duration")+ylab("centered f0 (ERB)"),

# this plot shows significant diffs overlaid on actual fits
#ggplot(HHH_HHL_fit,aes(x=prop_dur,y=fit))+ 
 # geom_rect(data=rects_HHH_HHL, inherit.aes=FALSE, aes(xmin=start, xmax=end, ymin=-10,ymax=10, group=1), color="transparent", fill="blue", alpha=0.15)+
  #coord_cartesian(ylim=c(-1,2.5))+
  #geom_ribbon(aes(ymin=ll,ymax=ul,fill=tune),alpha=0.2)+
  #geom_line(aes(color=tune))+xlab("prop. word duration")+ylab("centered f0 (ERB)")

ggplot(HHH_HHL,aes(x=prop_dur,y=est))+ geom_hline(yintercept=0,linetype=2,color="gray50")+
  geom_rect(data=rects_HHH_HHL, inherit.aes=FALSE, aes(xmin=start, xmax=end, ymin=-10,ymax=10, group=1), color="transparent", fill="blue", alpha=0.15)+
  coord_cartesian(ylim=c(-1,1))+
  geom_ribbon(aes(ymin=est-CI,ymax=est+CI,color=NULL),alpha=0.2)+
  geom_line()+
  ggtitle("Difference smooth")+
  xlab("prop. word duration")+ ylab("est. smooth difference"),

align="h",axis = "b",
nrow=1)
```

### Thinking about the results

In this particular example, the results tell us something interesting about these two tunes (for these 5 speakers).  We can see that they differ in the scaling of their final f0, where HHH is higher than HHL (in ToBI this would be H\*H-H% versus H\*H-L%). However we can also see that they vary in the shape of the f0 trajectory in the first portion of the word, around the pitch accent, with HHH showing a "scooped" rise, and HHL with a "domed" rise. This might have something to do with how these contour shapes displace so-called "Tonal Center of Gravity"; see e.g. @barnes2012tonal and @barnes2021and. 

In this sense, the GAMM has let us say something interesting about how speakers produce a distinction between these two tunes in capturing (non-linear) differences in contour shape. 

## Further exercises 

Here are some things you can do to extend what was covered above, using the RMarkdown code. 

* Fit a new model comparing two different tunes, say LHL and LLH, or two of your choosing, and characterize how they are different from one another. 

* Compare autocorrelation from this new model that you make to one which includes an AR1 error model. 

* Fit a new model with *all* of the tunes, and then examine pairwise comparisons of interest. 

* Do all of the above but using "difference smooths" hard-coded into the model - see the un-run code following the first high-rising tunes model above, and refer to @soskuthy2017generalised as needed. 


## References